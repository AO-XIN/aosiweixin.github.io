

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=&#34;auto&#34;>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="https://pic-aus-1252275196.cos.ap-nanjing.myqcloud.com/img/icon/IMG_0896%20%281%29.png">
  <link rel="icon" type="image/png" href="https://pic-aus-1252275196.cos.ap-nanjing.myqcloud.com/img/icon/IMG_0896%20%281%29.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="max">
  <meta name="keywords" content="">
  <title>Intership｜LLM应用从训练到构建 - Aoxin Blog</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.4.0/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" />
  



<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"www.auswitz.top","root":"/","version":"1.8.7","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"onlypost":false},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null}}};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.3.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>Aoxin`s Blog</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                简历
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                友链
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;</a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" href="javascript:">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('https://pic.downk.cc/item/5ff6ec6a3ffa7d37b3561203.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="Intership｜LLM应用从训练到构建">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2024-03-27 20:10" pubdate>
        2024年3月27日 晚上
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      3.2k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      40
       分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Intership｜LLM应用从训练到构建</h1>
            
            <div class="markdown-body">
              <p>以下有很多东西已脱密处理，可读性会降低。在咨询+1 得到同意后，把一些流程记录下来。</p>
<h1 id="1-LLM预训练中的参数设置以及训练设置"><a href="#1-LLM预训练中的参数设置以及训练设置" class="headerlink" title="1 LLM预训练中的参数设置以及训练设置"></a>1 LLM预训练中的参数设置以及训练设置</h1><h2 id="1-1-参数设置"><a href="#1-1-参数设置" class="headerlink" title="1.1 参数设置"></a>1.1 参数设置</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \ --stage sft \ --model_name_or_path /model/baichuan2/Baichuan2-7B-chat \ --do_train \ --dataset classify\ --template baichuan2 \ --finetuning_type lora \ --lora_target all \ --output_dir /cephfs/home/aooxin/LLM/LLM_model/classify_3_1_1 \ --overwrite_cache \ --per_device_train_batch_size 1 \ --per_device_eval_batch_size 1 \ --gradient_accumulation_steps 4 \ --lr_scheduler_type cosine \ --logging_steps 10 \ --save_steps 100 \ --lora_rank 64 \ --lora_alpha 128 \ --learning_rate 5e-5 \ --num_train_epochs 300 \ --plot_loss \ --fp16<br></code></pre></td></tr></table></figure>
<ul>
<li>stage：指令监督微调</li>
<li>model_name_or_path：模型我们选择Baichuan2</li>
<li>finetuning_type：选择lora</li>
<li>per_device_eval_batch_size：目前使用的3090只支持1 batch per gpu</li>
<li>learning_rate：经过多次尝试后，发现5e-5的学习率比较适合我们的任务</li>
<li>num_train_epochs：训练轮次可以根据数据量的多少来设置</li>
</ul>
<h2 id="1-2-训练加速"><a href="#1-2-训练加速" class="headerlink" title="1.2 训练加速"></a>1.2 训练加速</h2><p>加速可以选择<a target="_blank" rel="noopener" href="https://huggingface.co/docs/accelerate/index">Accelerate</a>、<a target="_blank" rel="noopener" href="https://github.com/microsoft/DeepSpeed">DeepSpeed</a><br>在这里我们选择了DeepSpeed，DeepSpeed可以通过zero技术增加通信代价来减少显存消耗，但是我们的3090每张卡可以单独放下一个batch，所以我们使用zero 0的设置，实际上就为普通的DDP多卡并行。不同Zero的设置大致效果如下：</p>
<blockquote>
<p>速度上：阶段 0 (DDP) &gt; 阶段 1 &gt; 阶段 2 &gt; 阶段 3<br>显存上：阶段 0 (DDP) &lt; 阶段 1 &lt; 阶段 2 &lt;  阶段 3 </p>
</blockquote>
<p>需要增加以下依赖</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">deepspeed==0.12.3<br>transformers==4.34.1<br>datasets==2.14.7<br>tiktoken==0.5.1<br>peft==0.6.2<br>trl==0.7.1<br></code></pre></td></tr></table></figure>
<p>zero 0的deepspeed配置文件如下</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs json">&#123;<br><br>    <span class="hljs-attr">&quot;bf16&quot;</span>: &#123;<br><br>        <span class="hljs-attr">&quot;enabled&quot;</span>: <span class="hljs-literal">false</span><br><br>    &#125;,<br><br>    <span class="hljs-attr">&quot;fp16&quot;</span>: &#123;<br><br>        <span class="hljs-attr">&quot;enabled&quot;</span>: <span class="hljs-literal">true</span><br><br>    &#125;,<br><br>    <span class="hljs-attr">&quot;zero_optimization&quot;</span>: &#123;<br><br>        <span class="hljs-attr">&quot;stage&quot;</span>: <span class="hljs-number">0</span><br><br>    &#125;,<br><br>    <span class="hljs-attr">&quot;gradient_clipping&quot;</span>: <span class="hljs-number">1.0</span>,<br><br>    <span class="hljs-attr">&quot;train_batch_size&quot;</span>: <span class="hljs-string">&quot;auto&quot;</span>,<br><br>    <span class="hljs-attr">&quot;train_micro_batch_size_per_gpu&quot;</span>: <span class="hljs-number">1</span>,<br><br>    <span class="hljs-attr">&quot;gradient_accumulation_steps&quot;</span>: <span class="hljs-number">4</span>,<br><br>    <span class="hljs-attr">&quot;steps_per_print&quot;</span>: <span class="hljs-number">2000000</span><br><br>&#125;<br></code></pre></td></tr></table></figure>
<p>使用deepspeed对应的训练启动命令也要修改，如果使用单机8卡的机器，可以使用以下命令启动</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">deepspeed --include=<span class="hljs-string">&quot;localhost:0,1,2,3,4,5,6,7&quot;</span> src/train_bash.py --stage sft --model_name_or_path /model/baichuan2/Baichuan2-7B-chat --do_train --dataset classify --template baichuan2 --finetuning_type lora --lora_target all --output_dir /cephfs/home/aooxin/LLM/LLM_model/classify_3_1_1 --overwrite_cache --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4 --lr_scheduler_type cosine --logging_steps 10 --save_steps 100 --lora_rank 64 --lora_alpha 128 --learning_rate 5e-5 --num_train_epochs 300 --plot_loss --fp16 --deepspeed /root/ds_config.json<br></code></pre></td></tr></table></figure>
<h2 id="1-3-完整一次训练流程"><a href="#1-3-完整一次训练流程" class="headerlink" title="1.3 完整一次训练流程"></a>1.3 完整一次训练流程</h2><h3 id="1-3-1-训练"><a href="#1-3-1-训练" class="headerlink" title="1.3.1 训练"></a>1.3.1 训练</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">deepspeed --include=<span class="hljs-string">&quot;localhost:0,1,2,3,4,5,6,7&quot;</span> src/train_bash.py --stage sft --model_name_or_path /model/baichuan2/Baichuan2-7B-chat --do_train --dataset classify --template baichuan2 --finetuning_type lora --lora_target all --output_dir /cephfs/home/aooxin/LLM/LLM_model/classify_3_1_1 --overwrite_cache --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4 --lr_scheduler_type cosine --logging_steps 10 --save_steps 100 --lora_rank 64 --lora_alpha 128 --learning_rate 5e-5 --num_train_epochs 300 --plot_loss --fp16 --deepspeed /root/ds_config.json<br></code></pre></td></tr></table></figure>
<h3 id="1-3-2-导出"><a href="#1-3-2-导出" class="headerlink" title="1.3.2 导出"></a>1.3.2 导出</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python src/export_model.py \ --model_name_or_path /model/baichuan2/Baichuan2-7B-chat \ --template baichuan2 \ --finetuning_type lora \ --checkpoint_dir /cephfs/home/aooxin/LLM/LLM_model/classify_3_2_p1check_p4/ \ --output_dir /cephfs/home/aooxin/LLM/LLM_model/model_classify_3_2_p1check_p4 \ --fp16<br></code></pre></td></tr></table></figure>
<h3 id="1-3-3-量化"><a href="#1-3-3-量化" class="headerlink" title="1.3.3 量化"></a>1.3.3 量化</h3><p>目前资源够使用，没有使用量化模型的步骤，如需，可以使用chatglm进行量化</p>
<h3 id="1-3-4-api部署"><a href="#1-3-4-api部署" class="headerlink" title="1.3.4 api部署"></a>1.3.4 api部署</h3><p>需要对api_demo.py进行修改，修改如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><br><span class="hljs-keyword">import</span> uvicorn<br><br><span class="hljs-keyword">from</span> llmtuner <span class="hljs-keyword">import</span> ChatModel, create_app<br><br>  <br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span>():</span><br><br>    chat_model = ChatModel()<br><br>    app = create_app(chat_model)<br><br>    port = <span class="hljs-built_in">int</span>(os.getenv(<span class="hljs-string">&quot;PORT&quot;</span>, <span class="hljs-number">8000</span>))<br><br>    uvicorn.run(app, host=<span class="hljs-string">&quot;0.0.0.0&quot;</span>, port=port, workers=<span class="hljs-number">1</span>)<br>    <br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br><br>    main()<br></code></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">PORT=8000 python src/api_demo.py \ --model_name_or_path /cephfs/home/aooxin/LLM/LLM_model/model_classify_3_8_p1check_p4check_str \ --template baichuan2 \ --finetuning_type lora<br></code></pre></td></tr></table></figure>
<h1 id="2-数据集构造"><a href="#2-数据集构造" class="headerlink" title="2 数据集构造"></a>2 数据集构造</h1><h2 id="2-1-构造样式"><a href="#2-1-构造样式" class="headerlink" title="2.1 构造样式"></a>2.1 构造样式</h2><p>由于我们实现的功能有几种，所以数据集的构造也有一定的细微差别，主要体现在input prompt上</p>
<h3 id="2-1-1-每种类型格式"><a href="#2-1-1-每种类型格式" class="headerlink" title="2.1.1 每种类型格式"></a>2.1.1 每种类型格式</h3><ul>
<li><p>prompt构造</p>
<p>  <strong>此处删除</strong></p>
<h3 id="2-1-2-构造原因"><a href="#2-1-2-构造原因" class="headerlink" title="2.1.2 构造原因"></a>2.1.2 构造原因</h3><p>分为3种</p>
</li>
<li><p>input中不需要额外prompt：比如classify和extract，这两种llm的实现不需要外界额外信息，因此不需要额外的prompt。</p>
</li>
<li><p>input中需要一种额外prompt：比如head和point，这两种的实现只需要少量外界额外信息，因此只需要补充上额外的信息即可。</p>
</li>
<li><p>input中需要两种额外prompt：比如line和surface，这两种军标中，每个小的军标都可能存在不同的算法，所以需要在额外基础信息的情况下，加上军标的补充说明去训练，比如下面这两种军标，存在完全不同的计算方法，所以针对某种军标需要单独设计数据去参与训练。</p>
<ul>
<li>作战分界线中，0为不含1为含</li>
<li>燕尾箭头中，通过起点、经过点、终点确定，由于起点需要两个点，所以需根据南北、东西走向确定，如果是南北，则在起点的经度上进行+-0.005；如果是东西，则在起点的纬度上+-0.005</li>
</ul>
</li>
</ul>
<h2 id="2-2-完整构造流程"><a href="#2-2-完整构造流程" class="headerlink" title="2.2 完整构造流程"></a>2.2 完整构造流程</h2><p>一次完整的构造流程分为生成数据-&gt;校准数据-&gt;转化数据</p>
<h3 id="2-2-1-生成数据"><a href="#2-2-1-生成数据" class="headerlink" title="2.2.1 生成数据"></a>2.2.1 生成数据</h3><p>生成数据我们使用<strong>gpt-4-turbo-preview</strong>，之所以使用gpt-4-turbo-preview是因为它相比于其他模型具有限定json格式输出的功能，其tokens长度也完全足够我们使用。与gpt4相比，他的单位tokens也更便宜，如下，gpt3.5没有限制json输出的功能，综合来讲选择gpt-4-turbo-preview比较合理。</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Input</th>
<th>Output</th>
</tr>
</thead>
<tbody><tr>
<td>gpt-4-0125-preview</td>
<td>$10.00 / 1M tokens</td>
<td>$30.00 / 1M tokens</td>
</tr>
<tr>
<td><strong>gpt-4-1106-preview</strong></td>
<td><strong>$10.00 / 1M tokens</strong></td>
<td><strong>$30.00 / 1M tokens</strong></td>
</tr>
<tr>
<td>gpt-4-1106-vision-preview</td>
<td>$10.00 / 1M tokens</td>
<td>$30.00 / 1M tokens</td>
</tr>
<tr>
<td>gpt-4</td>
<td>$30.00 / 1M tokens</td>
<td>$60.00 / 1M tokens</td>
</tr>
<tr>
<td>gpt-4-32k</td>
<td>$60.00 / 1M tokens</td>
<td>$120.00 / 1M tokens</td>
</tr>
<tr>
<td>gpt-3.5-turbo-0125</td>
<td>$0.50 / 1M tokens</td>
<td>$1.50 / 1M tokens</td>
</tr>
<tr>
<td>gpt-3.5-turbo-instruct</td>
<td>$1.50 / 1M tokens</td>
<td>$2.00 / 1M tokens</td>
</tr>
<tr>
<td>classify的生成代码如下，其余的代码在gitlab中</td>
<td></td>
<td></td>
</tr>
<tr>
<td>另外在生成数据的时候，可能要注意以下几点：</td>
<td></td>
<td></td>
</tr>
</tbody></table>
<ol>
<li><p>可以人工给出的信息，尽量提前给出：大模型可能会有很强的某种数据的倾向，尽管这些倾向有时候看起来匪夷所思，比如示例军标我们给出一个直升机，让他随机，他就可能倾向于战斗机等等的其他种类的飞机，而忽略其他军标，所以随机数据上减少对gpt的使用，比如ID可以使用uuid，军标可以通过python随机提前给出，这样gpt生成的效果会更好。</p>
</li>
<li><p>信息与信息有强相关性，就要一并给出：军标有中文和英文，生成数据的时候就要一并给出，不能让gpt自己猜测，防止生成的数据训练出来的模型未来也使用猜测的英文。</p>
</li>
<li><p>数学逻辑特别要注意：比如某点南北展开，对应我们的点需要纬度上+-0.005，这样的在生成的时候可以分开生成，先强制全部生成南北的，后续我们重新给出东西的例子，再重新生成东西。可以避免后续的矫正。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-keyword">import</span> os<br><br><span class="hljs-keyword">from</span> openai <span class="hljs-keyword">import</span> OpenAI<br><br><span class="hljs-keyword">from</span> dotenv <span class="hljs-keyword">import</span> load_dotenv<br><br><span class="hljs-keyword">import</span> json<br><br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><br>  <br>  <br><br><span class="hljs-comment"># Load environment variables</span><br><br>load_dotenv()<br><br>  <br><br><span class="hljs-comment"># Initialize OpenAI client</span><br><br>client = OpenAI(<br><br>    api_key=os.environ.get(<span class="hljs-string">&quot;OPENAI_API_KEY&quot;</span>),<br><br>)<br><br>folder_path = <span class="hljs-string">&#x27;data&#x27;</span><br><br>  <br><br><span class="hljs-comment"># Number of data entries to generate</span><br><br>n = <span class="hljs-number">50</span>  <span class="hljs-comment"># Adjust n to the desired number of data entries</span><br><br>  <br><br><span class="hljs-comment"># Loop to generate and print n data entries</span><br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> tqdm(<span class="hljs-built_in">range</span>(n)):<br><br>    <span class="hljs-comment"># Define the prompt for each iteration</span><br><br>    prompt = <span class="hljs-string">f&quot;&quot;&quot;</span><br><span class="hljs-string"></span><br><span class="hljs-string">    生成第<span class="hljs-subst">&#123;i+<span class="hljs-number">1</span>&#125;</span>条数据。以下是一个示例和指令：</span><br><span class="hljs-string"></span><br><span class="hljs-string">    我给你一个例子，需要生成新的json数据</span><br><span class="hljs-string"></span><br><span class="hljs-string">    &#123;&#123;</span><br><span class="hljs-string"></span><br><span class="hljs-string">    &quot;instruction&quot; : &quot;*******&quot;,</span><br><span class="hljs-string"></span><br><span class="hljs-string">    &quot;input&quot; : &quot;*********&quot;,</span><br><span class="hljs-string"></span><br><span class="hljs-string">    &quot;output&quot;:</span><br><span class="hljs-string"></span><br><span class="hljs-string">        &#123;&#123;</span><br><span class="hljs-string"></span><br><span class="hljs-string">            ******</span><br><span class="hljs-string"></span><br><span class="hljs-string">    &#125;&#125;</span><br><span class="hljs-string"></span><br><span class="hljs-string">    请生成更多这样的json数据,注意*****</span><br><span class="hljs-string"></span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>  <br><br>    <span class="hljs-comment"># Generate the completion for the current iteration</span><br><br>    chat_completion = client.chat.completions.create(<br><br>        messages=[<br><br>            &#123;<br><br>                <span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>,<br><br>                <span class="hljs-string">&quot;content&quot;</span>: prompt,<br><br>            &#125;<br><br>        ],<br><br>        model=<span class="hljs-string">&quot;gpt-4-turbo-preview&quot;</span>,<br><br>        response_format=&#123; <span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;json_object&quot;</span> &#125;,<br><br>    )<br><br>    file_path = os.path.join(folder_path, <span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;i+<span class="hljs-number">1</span>&#125;</span>.json&#x27;</span>)<br><br>    generated_content = chat_completion.choices[<span class="hljs-number">0</span>].message.content <span class="hljs-keyword">if</span> chat_completion.choices <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;No response&#x27;</span><br><br>    <span class="hljs-comment"># Save the generated data to a JSON file</span><br><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file_path, <span class="hljs-string">&#x27;w&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> file:<br><br>        <span class="hljs-comment"># Assuming generated_content is a string in JSON format, directly write it</span><br><br>        file.write(generated_content)<br><br></code></pre></td></tr></table></figure>
<p>生成好之后需要对数据进行拼接，将多个json拼接成一个完整的即可。</p>
</li>
</ol>
<h3 id="2-2-2-校准数据"><a href="#2-2-2-校准数据" class="headerlink" title="2.2.2 校准数据"></a>2.2.2 校准数据</h3><p>校准数据就需要按照每种数据的需求，对关键点进行人工判断，修改出错的地方。</p>
<h3 id="2-2-3-转化数据"><a href="#2-2-3-转化数据" class="headerlink" title="2.2.3 转化数据"></a>2.2.3 转化数据</h3><p>经过测试，如果直接将json作为output输入给训练程序效果会不好。<br>可以使用json.dumps将output转化为字符串即可。</p>
<h1 id="3-后端实现"><a href="#3-后端实现" class="headerlink" title="3 后端实现"></a>3 后端实现</h1><p>在langchain和直接使用代码自己实现功能的两种选择中，选择直接自己用python实现相关功能，原因是：一是langchain中的向量数据库的知识库形式并不精准，在我们的应用中不能起到好的效果，二是如果使用langchain调用api没有必要，白白增加了一层应用，不如自己手动调用。<br>这里列举几个langchain使用的例子：<strong>针对特定文档的问答</strong>：根据给定的文档回答问题，使用这些文档中的信息来创建答案。<strong>Agents</strong>：开发可以决定行动、采取这些行动、观察结果并继续执行直到完成的代理。可以看出langchain是可以完成多agents功能，但是目前我们的任务流水执行暂时不需要让模型去选择调用哪个api，数据是单向流动，未来如果考虑减少使用到的模型的数量可能就需要使用langchain来做。</p>
<h2 id="3-1-服务器api"><a href="#3-1-服务器api" class="headerlink" title="3.1 服务器api"></a>3.1 服务器api</h2><p>服务器api这里需要实现一些查询功能以及计算的后处理功能。我选择Flask实现</p>
<p>查询部分：<br>    查询部分的内容非常清楚，如下表</p>
<table>
<thead>
<tr>
<th>端点</th>
<th>方法</th>
<th>参数/请求体</th>
<th>返回内容</th>
<th>状态码</th>
</tr>
</thead>
<tbody><tr>
<td><code>/get_location</code></td>
<td>GET</td>
<td><code>address</code>: 地名列表，逗号分隔</td>
<td>JSON格式的地点信息列表</td>
<td>200: 成功<br>404: 未找到数据</td>
</tr>
<tr>
<td><code>/get_jb</code></td>
<td>GET</td>
<td><code>name</code>: 军标名称列表，逗号分隔</td>
<td>JSON格式的军标信息列表</td>
<td>200: 成功<br>404: 未找到数据</td>
</tr>
<tr>
<td><code>/add_location</code></td>
<td>POST</td>
<td>JSON: 包含<code>地名</code>、<code>经度</code>、<code>纬度</code>和<code>高度</code>字段</td>
<td>确认信息</td>
<td>201: 成功添加<br>400: 请求中缺少JSON或数据</td>
</tr>
<tr>
<td><code>/update_location</code></td>
<td>POST</td>
<td>JSON: 包含<code>地名</code>、<code>经度</code>、<code>纬度</code>和<code>高度</code>字段</td>
<td>确认信息</td>
<td>200: 成功更新<br>400: 请求中缺少JSON或数据<br>404: 未找到地点</td>
</tr>
<tr>
<td><code>/get_id</code></td>
<td>GET</td>
<td>无</td>
<td>JSON格式，包含生成的随机ID</td>
<td>200: 成功</td>
</tr>
</tbody></table>
<p>计算部分：<br>    计算部分我在这里举个例子：<strong>此处已删除</strong></p>
<h2 id="3-2-大模型api"><a href="#3-2-大模型api" class="headerlink" title="3.2 大模型api"></a>3.2 大模型api</h2><table>
<thead>
<tr>
<th>端点</th>
<th>方法</th>
<th>摘要</th>
<th>请求体需求</th>
<th>响应类型及数据结构</th>
</tr>
</thead>
<tbody><tr>
<td><code>/v1/models</code></td>
<td>GET</td>
<td>列出模型</td>
<td>无</td>
<td>200: 成功，返回<code>ModelList</code></td>
</tr>
<tr>
<td><code>/v1/chat/completions</code></td>
<td>POST</td>
<td>创建聊天完成</td>
<td>需要，<code>ChatCompletionRequest</code></td>
<td>200: 成功，返回<code>ChatCompletionResponse</code><br>422: 验证错误</td>
</tr>
</tbody></table>
<p>使用python对大模型进行调用即可</p>
<h2 id="3-3-本地python"><a href="#3-3-本地python" class="headerlink" title="3.3 本地python"></a>3.3 本地python</h2><p>本地python对于各个大模型、服务器api按照流水进行使用即可，可以注意以下几点：</p>
<ol>
<li><p>在写整体的流水线处理的时候，因为每次调用都是<strong>输入文本-&gt;扩展prompt+调用插件-&gt;调用llm-&gt;结果的后处理</strong>。可以注意代码的复用，避免重复，未来也可以更好地扩展。</p>
</li>
<li><p>因为每次调用模型都要花费时间，所以加入日志记录模块可以很快的找到是输入、扩展prompt还是后处理哪一步出错了，日志中可以按我们处理的流程记录每一个文件生成的经过。</p>
</li>
<li><p>在遇到：营指挥所、指挥所 这种选择问题，可以考虑加入分词，比如jieba。预先创建分词数据库，使用最长前缀匹配可以很好的解决这种问题。在这个应用中，可以想到分词的功能可能不止这点，比如说在最初的文本输入后，实际上可以使用传统分词将所有的军标先提取出来，这样我们就得到了军标们和最初文本，然后每一次输入大模型的都是完整的文本，和军标之一，这样可以巧妙避免前后文相关的情况，但是同时对数据集的构造制造了更大的难度。</p>
</li>
</ol>
<h1 id="4-不足"><a href="#4-不足" class="headerlink" title="4 不足"></a>4 不足</h1><ol>
<li><strong>数据集数量影响模型效果</strong>：特别是在line、surface中，不同军标都需要有一部分自己的数据集，生成合理的数据集是一个庞大的任务。</li>
<li><strong>每种计算需要单独设计z</strong>：对于坐标的复杂计算，需要单独设计计算方式。</li>
<li><strong>需要再增加一个llm专门用于对坐标的解释</strong>：比如有的坐标和南北方位有关，但是南北的表示又有很多不同，所以需要设计一个llm对类似的坐标方位进行一个固定的转化，目前demo阶段并未设计实现。</li>
<li><strong>需要一种统一的文件</strong>：llm的表现和数据集的形式强相关，如果换一种文件输入，原来的数据集就需要重新生成，导致应用可用性会降低。</li>
</ol>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/Intership/">Intership</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/LLM/">LLM</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！</p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2024/04/08/cu2/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">小技巧｜异构计算中C++ host 部分的性能测试（普通C++程序也适用）</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/03/17/cu-1/">
                        <span class="hidden-mobile">Notebook｜CUDA学习笔记</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
              <!-- Comments -->
              <article class="comments" id="comments">
                
                
  <div id="vcomments"></div>
  <script type="text/javascript">
    Fluid.utils.waitElementVisible('vcomments', function() {
      Fluid.utils.createScript('https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js', function () {
        new Valine({
          el: "#vcomments",
          app_id: "NWytlCMJI03CQOQYOFuI0nNx-gzGzoHsz",
          app_key: "dfBdcYBLVYgWf12l7QnVV3wC",
          placeholder: "说点什么",
          path: window.location.pathname,
          avatar: "retro",
          meta: ["nick","mail","link"],
          pageSize: "10",
          lang: "zh-CN",
          highlight: false,
          recordIP: false,
          serverURLs: "",
        });
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the
    <a target="_blank" href="https://valine.js.org" rel="nofollow noopener noopener">comments powered by Valine.</a>
  </noscript>


              </article>
            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://www.auswitz.top" target="_blank" rel="nofollow noopener"><span>Aus</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  
  <div class="statistics">
    
    

    
      
        <!-- 不蒜子统计PV -->
        <span id="busuanzi_container_site_pv" style="display: none">
            总访问量 
            <span id="busuanzi_value_site_pv"></span>
             次
          </span>
      
      
        <!-- 不蒜子统计UV -->
        <span id="busuanzi_container_site_uv" style="display: none">
            总访客数 
            <span id="busuanzi_value_site_uv"></span>
             人
          </span>
      
    
  </div>


  

  
</footer>

<!-- SCRIPTS -->

  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.min.js" ></script>
<script  src="/js/debouncer.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/lazyload.js" ></script>
  



  



  <script  src="https://cdn.jsdelivr.net/npm/tocbot@4.12.0/dist/tocbot.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.0/anchor.min.js" ></script>



  <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.6/dist/clipboard.min.js" ></script>



  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    (function () {
      var path = "/local-search.xml";
      var inputArea = document.querySelector("#local-search-input");
      inputArea.onclick = function () {
        searchFunc(path, 'local-search-input', 'local-search-result');
        this.onclick = null
      }
    })()
  </script>















<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>



<script type="text/javascript" src="\js\snow.js"></script>
<script type="text/javascript" src="\js\click.js"></script>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":140,"height":260},"mobile":{"show":true},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
